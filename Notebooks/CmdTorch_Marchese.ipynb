{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "782fc82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/CAMPUS/cemb2020/anaconda3/envs/arcslab/lib/python3.8/site-packages/fastbook/__init__.py:18: UserWarning: Missing `graphviz` - please run `conda install fastbook`\n",
      "  except ModuleNotFoundError: warn(\"Missing `graphviz` - please run `conda install fastbook`\")\n"
     ]
    }
   ],
   "source": [
    "# Import appropriate libraries and packages\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import fastbook\n",
    "fastbook.setup_book()\n",
    "from fastbook import *\n",
    "from fastai.vision.widgets import *\n",
    "from cmd_classes_funcs_Marchese import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a61d2810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set GPU to run on if available\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec812a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get classes and filenames\n",
    "path = Path(\"data\")\n",
    "classes = get_class_labels(path)\n",
    "all_filenames = get_filenames(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80186b5",
   "metadata": {},
   "source": [
    "## Splitting Data into train/validate sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6329781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting size of dataset and corresponding list of indices\n",
    "dataset_size = len(all_filenames)\n",
    "dataset_indices = list(range(dataset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5946f9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the indices\n",
    "np.random.shuffle(dataset_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "792b1041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting index for where we want to split the data\n",
    "val_split_index = int(np.floor(0.2 * dataset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44c842e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting list of indices into training and validation indices\n",
    "train_idx, val_idx = dataset_indices[val_split_index:], dataset_indices[:val_split_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b219033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating samplers\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adda3686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting list of filenames for training and validation set\n",
    "train_filenames = [all_filenames[i] for i in train_idx]\n",
    "val_filenames = [all_filenames[i] for i in val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c15aa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation datasets\n",
    "train_data = ImageWithCmdDataset(classes, train_filenames)\n",
    "val_data = ImageWithCmdDataset(classes, val_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67a0a805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training set and validation set data loaders\n",
    "train_loader = DataLoader(dataset=train_data, shuffle=False, batch_size=16, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset=val_data, shuffle=False, batch_size=16, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2808ebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the network\n",
    "net = MyModel_next50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f30881c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel_next50(\n",
       "  (cnn): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "  )\n",
       "  (bn1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dr1): Dropout(p=0.25, inplace=False)\n",
       "  (fc1): Linear(in_features=1001, out_features=512, bias=True)\n",
       "  (r1): ReLU(inplace=True)\n",
       "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dr2): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=512, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Send model to GPU\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bf101ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "####n = dataset_size\n",
    "#### w = torch.tensor([(n-138)/n,(n-211)/n,(n-786)/n])\n",
    "#### w = w.to(device)\n",
    "# defining loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=w)\n",
    "optimizer = optim.Adam(net.parameters(), lr=.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bb5053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e45f279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import time package to keep track of training time\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bc0ba25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/50, Training Loss:185.2, Time:77.5s\n",
      "Epoch:2/50, Training Loss:122.7, Time:76.6s\n",
      "Epoch:3/50, Training Loss:91.5, Time:76.0s\n",
      "Epoch:4/50, Training Loss:65.9, Time:77.4s\n",
      "Epoch:5/50, Training Loss:48.5, Time:77.2s\n",
      "Epoch:6/50, Training Loss:40.7, Time:77.3s\n",
      "Epoch:7/50, Training Loss:29.3, Time:77.3s\n",
      "Epoch:8/50, Training Loss:29.2, Time:77.4s\n",
      "Epoch:9/50, Training Loss:33.0, Time:77.9s\n",
      "Epoch:10/50, Training Loss:30.0, Time:76.4s\n",
      "Epoch:11/50, Training Loss:18.3, Time:76.8s\n",
      "Epoch:12/50, Training Loss:21.7, Time:77.1s\n",
      "Epoch:13/50, Training Loss:19.1, Time:76.7s\n",
      "Epoch:14/50, Training Loss:14.3, Time:76.7s\n",
      "Epoch:15/50, Training Loss:13.8, Time:76.8s\n",
      "Epoch:16/50, Training Loss:11.8, Time:76.6s\n",
      "Epoch:17/50, Training Loss:18.4, Time:76.7s\n",
      "Epoch:18/50, Training Loss:12.5, Time:77.2s\n",
      "Epoch:19/50, Training Loss:8.1, Time:77.0s\n",
      "Epoch:20/50, Training Loss:6.5, Time:77.7s\n",
      "Epoch:21/50, Training Loss:11.1, Time:78.0s\n",
      "Epoch:22/50, Training Loss:17.8, Time:77.1s\n",
      "Epoch:23/50, Training Loss:12.8, Time:77.0s\n",
      "Epoch:24/50, Training Loss:10.5, Time:77.5s\n",
      "Epoch:25/50, Training Loss:6.9, Time:77.5s\n",
      "Epoch:26/50, Training Loss:9.2, Time:78.7s\n",
      "Epoch:27/50, Training Loss:16.0, Time:77.8s\n",
      "Epoch:28/50, Training Loss:6.4, Time:78.2s\n",
      "Epoch:29/50, Training Loss:6.4, Time:78.7s\n",
      "Epoch:30/50, Training Loss:2.1, Time:78.7s\n",
      "Epoch:31/50, Training Loss:1.9, Time:78.4s\n",
      "Epoch:32/50, Training Loss:6.2, Time:77.2s\n",
      "Epoch:33/50, Training Loss:26.4, Time:77.9s\n",
      "Epoch:34/50, Training Loss:8.0, Time:77.7s\n",
      "Epoch:35/50, Training Loss:5.4, Time:77.6s\n",
      "Epoch:36/50, Training Loss:3.3, Time:77.3s\n",
      "Epoch:37/50, Training Loss:12.2, Time:77.4s\n",
      "Epoch:38/50, Training Loss:4.5, Time:77.3s\n",
      "Epoch:39/50, Training Loss:0.9, Time:77.9s\n",
      "Epoch:40/50, Training Loss:0.9, Time:78.0s\n",
      "Epoch:41/50, Training Loss:0.5, Time:78.2s\n",
      "Epoch:42/50, Training Loss:0.4, Time:78.0s\n",
      "Epoch:43/50, Training Loss:0.4, Time:78.1s\n",
      "Epoch:44/50, Training Loss:0.2, Time:77.8s\n",
      "Epoch:45/50, Training Loss:0.3, Time:77.5s\n",
      "Epoch:46/50, Training Loss:0.2, Time:78.0s\n",
      "Epoch:47/50, Training Loss:0.4, Time:77.9s\n",
      "Epoch:48/50, Training Loss:0.1, Time:77.9s\n",
      "Epoch:49/50, Training Loss:0.1, Time:78.6s\n",
      "Epoch:50/50, Training Loss:0.1, Time:78.0s\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "\n",
    "net.train()\n",
    "\n",
    "for epoch in range(num_epochs):  \n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    for data in train_loader:\n",
    "        # Get the inputs and labels\n",
    "        inp_data, label = data\n",
    "        \n",
    "        # Break up the inputs\n",
    "        img, cmd = inp_data\n",
    "        \n",
    "        # Putting data into the GPU\n",
    "        img = img.to(device)\n",
    "        cmd = cmd.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward + backward + optimize\n",
    "        output = net((img, cmd))\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add loss of current data to running loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    # Print statistics    \n",
    "    print(f\"Epoch:{epoch+1:}/{num_epochs}, Training Loss:{running_loss:0.1f}, Time:{time()-start:0.1f}s\")\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "deb3975e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set: 1297/1399 = 92.71%\n",
      "  Accuracy on  left class: 192/227 = 84.58%\n",
      "  Accuracy on right class: 201/243 = 82.72%\n",
      "  Accuracy on straight class: 904/929 = 97.31%\n"
     ]
    }
   ],
   "source": [
    "# Checking accuracy on validation set\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Variables to keep track of accuracy for each class\n",
    "class_correct = [0 for _ in classes]\n",
    "class_total = [0 for _ in classes]\n",
    "\n",
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for data in val_loader:\n",
    "\n",
    "        # Get the inputs and label data\n",
    "        inp_data, label = data\n",
    "        \n",
    "        # Break up the inputs\n",
    "        img, cmd = inp_data\n",
    "        \n",
    "        # Put data into the GPU\n",
    "        img = img.to(device)\n",
    "        cmd = cmd.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "\n",
    "        # Predict\n",
    "        output = net((img, cmd))\n",
    "        \n",
    "        # Assuming we always get batches\n",
    "        for i in range(output.size()[0]):\n",
    "                \n",
    "            # Get the predicted most probable move\n",
    "            move = torch.argmax(output[i])\n",
    "                \n",
    "            if move == label[i]:\n",
    "                class_correct[label[i]] += 1\n",
    "                class_total[label[i]] += 1\n",
    "                correct +=1\n",
    "            else:\n",
    "                class_total[label[i]] += 1\n",
    "            total += 1\n",
    "        \n",
    "# Calculate and output total set accuracy \n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy on validation set: {correct}/{total} = {accuracy*100:.2f}%\")\n",
    "\n",
    "# Calculate and show accuracy for each class\n",
    "for i, cls in enumerate(classes):\n",
    "    ccorrect = class_correct[i]\n",
    "    ctotal = class_total[i]\n",
    "    caccuracy = ccorrect / ctotal\n",
    "    print(f\"  Accuracy on {cls:>5} class: {ccorrect}/{ctotal} = {caccuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f369beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model given PATH\n",
    "PATH = 'cmd_torch_next50_.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
